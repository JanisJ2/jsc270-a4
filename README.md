# JSC270 Winter 2024 Assignment 4: Natural Language Processing (NLP)

**Team Members:**  
- Christoffer Tan
- Janis Joplin

---

## Contributions Breakdown

This assignment was completed collaboratively, with both team members actively contributing to every aspect of the work. We worked jointly on code development, report writing, and presentation creation. The following tools were used:
- **Git** for code sharing
- **Typst** for report writing
- **PowerPoint** for presentation creation

---

## Part I: Sentiment Analysis with a Twitter Dataset

### A) Sentiment Proportions  
- Based on the training data, the proportions for each sentiment type are provided (sentiment type 0: negative, type 1: neutral, type 2: positive).

### B) Tokenization  
- Tweets were tokenized using the `punkt` tokenizer from the `nltk` library. Example tokens for each tweet are displayed.

### C) URL Removal  
- The `remove_url` function iterates through each row of the `Tokens` column, using a regular expression (via the `re` library) to delete all tokens that are URLs.

### D) Punctuation Removal and Lowercasing  
- The `remove_punctuation` function processes each row in the `Tokens` column by using a regular expression to eliminate all punctuation and special characters.
- The `convert_to_lowercase` function converts each token in the `Tokens` column to lowercase.

### E) Stemming  
- The `stemming_tokens` function uses `PorterStemmer` to reduce each token to its stem.
- Stemmed tokens are stored in a new list (`stemmed_tokens`), which then replaces the original data in the `Tokens` column.

### F) Stopwords Removal  
- The `remove_stopwords` function iterates through each row in the `Tokens` column, keeping only words that are not stopwords.

### G) Vocabulary  
- The length of the vocabulary is **74,225**.

### H) Naive Bayes Model Evaluation  
- The Naive Bayes model was applied to the data, with accuracy scores reported for both training and test datasets.
- The 5 most probable words in each sentiment class, along with their counts, are provided.

### I) Evaluation Metrics  
- It is noted that the ROC curve may not be appropriate here because the task involves three sentiment types, not just a binary classification. Alternative metrics and evaluation techniques are suggested.

### J) TF-IDF Transformer Comparison  
- After applying the TF-IDF transformer, accuracy scores for training and test data are reported.
- The results indicate that using count vectors yields better accuracy than the TF-IDF transformer.

### K) Lemmatization vs. Stemming  
- Replacing stemming with lemmatization produced slightly better accuracy on both training and test datasets.

### Bonus Explanation  
- The Naive Bayes model is described as generative. It models $Pr(w_1, …, w_D \mid S)$ to infer $Pr(S \mid w_1, …, w_D)$ where $w_1, …, w_D$ are the tokens in the document and $S$ is the sentiment (0, 1, or 2).  
- The process involves estimating the probability of a token given a sentiment, then using Bayes' rule to infer the probability of the sentiment given the tokens, selecting the sentiment with the highest probability.

---

## Part II: Trending Topic Analysis of Twitter Data using Latent Dirichlet Allocation and ChatGPT

### Problem Description and Motivation
- The rapid growth of Twitter has transformed information sharing and trend analysis.
- Challenges include dynamic trends, vast data volumes, and noisy discussions.
- The dataset consists of 10,000 tweets collected over approximately 4 days.
- Existing studies have used sentiment analysis and network analysis; our approach employs Latent Dirichlet Allocation (LDA) to uncover latent topics.

### Data Description
- The dataset (tweets from March 23–27, 2023) contains 10,000 observations with 7 features.
- The `tweet_text` field is the primary focus.
- Preprocessing steps include:
  - Expanding contractions
  - Removing mentions and tags
  - Tokenizing the tweet
  - Removing URLs and punctuation
  - Converting tokens to lowercase
  - Removing stopwords
  - Lemmatizing tokens
- **Limitation:** Some tweet messages are incomplete (cut off), potentially obscuring full context.

### Exploratory Data Analysis
- Only the `tweet_text` field was used for modeling.
- Preprocessing was followed by visualizations (word clouds and histograms).
- Observations include a significant number of tweets related to politics and government around Canada, possibly indicating notable events.

### Machine Learning Model
- **Latent Dirichlet Allocation (LDA):**
  - An unsupervised clustering model used for topic modeling.
  - Assumes each document is a mixture of topics, with words generated by these topics.
  - The model iteratively assigns words to topics to optimize the likelihood of the observed data.
- **Model Construction:**
  - The dataset was split into training (60%), validating (20%), and testing (20%) sets.
  - Topic coherence scores were used on the validation set to identify the optimal number of topics.
  - The optimal number of topics was found to be **13**.
  - The LDA model was built using Gensim's `ldamodel` with selected hyperparameters.
  - Generated topics were labeled using ChatGPT based on the relevant words and their scores.
  - The test data was then clustered based on the highest probability topics, with labels assigned from ChatGPT.

### Conclusion
- The LDA model, combined with ChatGPT for topic labeling, efficiently assigns topics to tweet messages.
- The model generates topics rapidly (often in under one second) and achieved a topic coherence score of **0.576** on the test dataset.
- This performance is comparable to baseline LDA models (topic coherence scores ranging from 0.5 to 0.59).
- **Limitations:**  
  - Occasional assignment of irrelevant topics to short tweets.
  - Difficulty with tweets that differ significantly from the training data.
  - Absence of bigram interpretation may result in loss of nuanced meaning.
- Despite these challenges, the model represents a significant advancement in topic analysis and has the potential to unlock deeper insights from social media content with further refinement.

---

## References

- **Bagheri, H. and Islam, Md (n.d).** *Sentiment analysis of twitter data.* Iowa State University, United States of America.
- **iCAS Data Science & Analytics Forum (2023).** *Latent Dirichlet Allocation (LDA) Topic Modelling in Python.* Retrieved from [iCAS-4 LDA Topic Modeling in Python](https://www.casact.org/sites/default/files/2023-04/iCAS-4_Latent_Dirichlet_Allocation_Topic_Modeling_in_Python.pdf).
- **Kapadia, S. (2019).** *Topic Modeling in Python: Latent Dirichlet Allocation (LDA).* Retrieved from [End-to-End Topic Modeling in Python](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0).
- **Li, S. (2018).** *Topic Modeling and Latent Dirichlet Allocation (LDA) in Python.* Retrieved from [Topic Modeling and LDA in Python](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24).

---
